{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb518ed7-0cff-4b2d-88a7-422a1b6f9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SDCAClassifier:\n",
    "    def __init__(self, loss=\"squared\", lam=1e-3, max_iter=1500, tol=1e-5, verbose=False, use_preSGD=True):\n",
    "        self.loss = loss\n",
    "        self.lam = lam\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.use_preSGD = use_preSGD\n",
    "        self.w = None\n",
    "        self.alpha = None\n",
    "        self.losses_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_internal = np.where(y <= 0, -1, 1) if self.loss == \"log\" else y.copy()\n",
    "\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        self.w = np.zeros(n_features)\n",
    "        alpha_x_sum = np.zeros(n_features)\n",
    "        start_epoch = 0\n",
    "\n",
    "        # --- FIRST EPOCH: MODIFIED-SGD (PER SCREENSHOT) ---\n",
    "        if self.use_preSGD:\n",
    "            if self.verbose: print(\"Running Modified-SGD (Epoch 1)...\")\n",
    "            \n",
    "            for t_idx in range(n_samples):\n",
    "                t = t_idx + 1  # Using t normalization as per ss\n",
    "                xi = X[t_idx]\n",
    "                yi = y_internal[t_idx]\n",
    "                norm_sq = np.dot(xi, xi)\n",
    "                \n",
    "                if norm_sq == 0: \n",
    "                    # If feature is zero, w remains unchanged, alpha_t is effectively 0\n",
    "                    continue\n",
    "\n",
    "                # Prediction using previous w: w^(t-1)\n",
    "                pred = np.dot(self.w, xi)\n",
    "                \n",
    "                # Denominator for solving the maximization in the box:\n",
    "                # Based on the screenshot: Find alpha_t to maximize -phi*(-alpha) - (lam*t/2) ||w + (1/lam*t) alpha*x||^2\n",
    "                # This leads to a denominator using (lam * t)\n",
    "                denom = norm_sq / (self.lam * t) + 1\n",
    "                \n",
    "                if self.loss == \"squared\":\n",
    "                    # alpha_t = (y_t - x_t @ w^(t-1)) / (||x_t||^2 / (lambda * t) + 1)\n",
    "                    alpha_t = (yi - pred) / denom\n",
    "                elif self.loss == \"log\":\n",
    "                    exp_term = np.exp(np.clip(yi * pred, -50, 50))\n",
    "                    grad = yi / (1 + exp_term)\n",
    "                    alpha_t = np.clip(grad / denom, min(0, yi), max(0, yi))\n",
    "\n",
    "                self.alpha[t_idx] = alpha_t\n",
    "                alpha_x_sum += alpha_t * xi\n",
    "                \n",
    "                # Update w based on the screenshot: w = (1 / lam*t) * sum_{i=1 to t} alpha_i*xi\n",
    "                self.w = alpha_x_sum / (self.lam * t)\n",
    "            print(\"Finnished Modified-SGD (Epoch 1)\")\n",
    "\n",
    "            start_epoch = 1\n",
    "            \n",
    "        # --- REMAINING EPOCHS: CLASSIC SDCA (NORMALIZED BY n) ---\n",
    "        for it in range(start_epoch, self.max_iter):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            for i in indices:\n",
    "                xi = X[i]\n",
    "                yi = y_internal[i]\n",
    "                norm_sq = np.dot(xi, xi)\n",
    "                if norm_sq == 0: continue\n",
    "\n",
    "                pred = np.dot(self.w, xi)\n",
    "                # Classic SDCA uses full n normalization\n",
    "                denom = norm_sq / (self.lam * n_samples) + 1\n",
    "                \n",
    "                if self.loss == \"squared\":\n",
    "                    delta = (yi - pred - self.alpha[i]) / denom\n",
    "                elif self.loss == \"log\":\n",
    "                    exp_term = np.exp(np.clip(yi * pred, -50, 50))\n",
    "                    grad = yi / (1 + exp_term) - self.alpha[i]\n",
    "                    delta = grad / denom\n",
    "                    new_alpha = np.clip(self.alpha[i] + delta, min(0, yi), max(0, yi))\n",
    "                    delta = new_alpha - self.alpha[i]\n",
    "\n",
    "                self.alpha[i] += delta\n",
    "                self.w += (delta / (self.lam * n_samples)) * xi\n",
    "            \n",
    "            primal_obj, dual_obj = self._compute_objectives(X, y_internal)\n",
    "            gap = primal_obj - dual_obj\n",
    "            self.losses_.append(primal_obj)\n",
    "\n",
    "            if self.verbose and (it + 1) % 10 == 0:\n",
    "                print(f\"Iter {it+1:3d} | Gap: {gap:.2e} | Primal Loss: {primal_obj:.6f}\")\n",
    "            \n",
    "            if gap < self.tol:\n",
    "                break\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def _compute_objectives(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        preds = X @ self.w\n",
    "        if self.loss == \"squared\":\n",
    "            loss_term = np.mean(0.5 * (preds - y)**2)\n",
    "            dual_res = np.mean(0.5 * self.alpha**2 - y * self.alpha)\n",
    "        else:\n",
    "            loss_term = np.mean(np.log(1 + np.exp(-y * preds)))\n",
    "            v = np.clip(y * self.alpha, 1e-12, 1 - 1e-12)\n",
    "            dual_res = np.mean(v * np.log(v) + (1 - v) * np.log(1 - v))\n",
    "            \n",
    "        primal_obj = loss_term + 0.5 * self.lam * np.dot(self.w, self.w)\n",
    "        dual_obj = -dual_res - 0.5 * self.lam * np.dot(self.w, self.w)\n",
    "        return primal_obj, dual_obj\n",
    " \n",
    "    def _compute_loss(self, X, y):\n",
    "        pred = X @ self.w \n",
    "        if self.loss == \"log\":\n",
    "            z = y * pred\n",
    "            return np.mean(np.log(1 + np.exp(-z))) + 0.5 * self.lam * np.sum(self.w**2)\n",
    "        else:\n",
    "            return np.mean((y - pred) ** 2) + 0.5 * self.lam * np.sum(self.w**2)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return (X @ self.w > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SDCAClassifier1:\n",
    "    def __init__(self, loss=\"squared\", lam=1e-3, max_iter=10000, tol=1e-5, verbose=False, use_preSGD=True):\n",
    "        self.loss = loss\n",
    "        self.lam = lam\n",
    "        self.max_iter = max_iter  # Now total number of individual updates\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.use_preSGD = use_preSGD\n",
    "        self.w = None\n",
    "        self.alpha = None\n",
    "        self.losses_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_internal = np.where(y <= 0, -1, 1) if self.loss == \"log\" else y.copy()\n",
    "\n",
    "        # Initialize: w^(0) = 0\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        self.w = np.zeros(n_features)\n",
    "        alpha_x_sum = np.zeros(n_features)\n",
    "        \n",
    "        current_step = 0\n",
    "\n",
    "        # --- PRE-STEP: Sequential Modified-SGD (1 update per step) ---\n",
    "        if self.use_preSGD:\n",
    "            if self.verbose: print(\"Running Modified-SGD Phase...\")\n",
    "            for t_idx in range(n_samples):\n",
    "                t = t_idx + 1\n",
    "                xi, yi = X[t_idx], y_internal[t_idx]\n",
    "                norm_sq = np.dot(xi, xi)\n",
    "                \n",
    "                if norm_sq > 0:\n",
    "                    pred = np.dot(self.w, xi)\n",
    "                    # Denominator uses lambda * t\n",
    "                    denom = norm_sq / (self.lam * t) + 1\n",
    "                    \n",
    "                    if self.loss == \"squared\":\n",
    "                        alpha_t = (yi - pred) / denom\n",
    "                    elif self.loss == \"log\":\n",
    "                        exp_term = np.exp(np.clip(yi * pred, -50, 50))\n",
    "                        grad = yi / (1 + exp_term)\n",
    "                        alpha_t = np.clip(grad / denom, min(0, yi), max(0, yi))\n",
    "\n",
    "                    self.alpha[t_idx] = alpha_t\n",
    "                    alpha_x_sum += alpha_t * xi\n",
    "                    # Update w = (1/lambda*t) * sum\n",
    "                    self.w = alpha_x_sum / (self.lam * t)\n",
    "                \n",
    "                current_step += 1\n",
    "                \n",
    "        # --- MAIN PHASE: Random SDCA (1 update per step) ---\n",
    "        # We continue from current_step until max_iter\n",
    "        if self.verbose: print(f\"Starting Main SDCA Phase from step {current_step}...\")\n",
    "        \n",
    "        for step in range(current_step, self.max_iter):\n",
    "            # Select exactly one random index\n",
    "            i = np.random.randint(n_samples)\n",
    "            xi, yi = X[i], y_internal[i]\n",
    "            norm_sq = np.dot(xi, xi)\n",
    "            \n",
    "            if norm_sq > 0:\n",
    "                pred = np.dot(self.w, xi)\n",
    "                # Standard SDCA uses lambda * n\n",
    "                denom = norm_sq / (self.lam * n_samples) + 1\n",
    "                \n",
    "                if self.loss == \"squared\":\n",
    "                    delta = (yi - pred - self.alpha[i]) / denom\n",
    "                elif self.loss == \"log\":\n",
    "                    exp_term = np.exp(np.clip(yi * pred, -50, 50))\n",
    "                    grad = yi / (1 + exp_term) - self.alpha[i]\n",
    "                    delta = grad / denom\n",
    "                    new_alpha = np.clip(self.alpha[i] + delta, min(0, yi), max(0, yi))\n",
    "                    delta = new_alpha - self.alpha[i]\n",
    "\n",
    "                self.alpha[i] += delta\n",
    "                # Incrementally update w\n",
    "                self.w += (delta / (self.lam * n_samples)) * xi\n",
    "\n",
    "            # Periodic convergence check (calculating objectives is expensive)\n",
    "            if step % n_samples == 0:\n",
    "                primal_obj, dual_obj = self._compute_objectives(X, y_internal)\n",
    "                gap = primal_obj - dual_obj\n",
    "                self.losses_.append(primal_obj)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"Step {step:6d} | Gap: {gap:.2e} | Primal: {primal_obj:.6f}\")\n",
    "                \n",
    "                if gap < self.tol:\n",
    "                    if self.verbose: print(f\"Converged at step {step}\")\n",
    "                    break\n",
    "                    \n",
    "        return self\n",
    "\n",
    "    def _compute_objectives(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        preds = X @ self.w\n",
    "        if self.loss == \"squared\":\n",
    "            loss_term = np.mean(0.5 * (preds - y)**2)\n",
    "            dual_res = np.mean(0.5 * self.alpha**2 - y * self.alpha)\n",
    "        else:\n",
    "            loss_term = np.mean(np.log(1 + np.exp(-y * preds)))\n",
    "            v = np.clip(y * self.alpha, 1e-12, 1 - 1e-12)\n",
    "            dual_res = np.mean(v * np.log(v) + (1 - v) * np.log(1 - v))\n",
    "            \n",
    "        primal_obj = loss_term + 0.5 * self.lam * np.dot(self.w, self.w)\n",
    "        dual_obj = -dual_res - 0.5 * self.lam * np.dot(self.w, self.w)\n",
    "        return primal_obj, dual_obj\n",
    "    \n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        pred = X @ self.w \n",
    "        if self.loss == \"log\":\n",
    "            z = y * pred\n",
    "            return np.mean(np.log(1 + np.exp(-z))) + 0.5 * self.lam * np.sum(self.w**2)\n",
    "        else:\n",
    "            return np.mean((y - pred) ** 2) + 0.5 * self.lam * np.sum(self.w**2)\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return (X @ self.w > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f12c0a74-997e-4ae3-b9c5-3f2e9d67d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset\n",
    "bank_marketing = fetch_ucirepo(id=222)\n",
    "X = bank_marketing.data.features\n",
    "y = (bank_marketing.data.targets == \"yes\").astype(int)  # convert yes/no -> 1/0\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y.values.ravel(), test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a50dba40-87af-4b03-bf0e-4f769cbcd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lr_set = [1]\n",
    "lams = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "lams.reverse()\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for lr in lr_set:\n",
    "    for lam in lams:\n",
    "        classifier = SDCAClassifier(loss=\"squared\", lam=lam, verbose=0, tol=1e-6)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        test_loss = classifier._compute_loss(X_test, y_test)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"LR {lr:.1e} - Lambda {lam:.1e} - Test Loss: {test_loss:.6f}- Steps {len(classifier.losses_)} - Accuracy {accuracy}\")\n",
    "        \n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_params = (lr, lam) \n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(\"LR:\", best_params[0], \"Lambda:\", best_params[1], \"Test Loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc91a7-42c4-499c-8d57-75a22114041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Modified-SGD (Epoch 1)...\n",
      "Finnished Modified-SGD (Epoch 1)\n",
      "Iter  10 | Gap: 3.12e-03 | Primal Loss: 0.616186\n",
      "Iter  20 | Gap: 4.27e-04 | Primal Loss: 0.615847\n",
      "Iter  30 | Gap: 1.93e-04 | Primal Loss: 0.615823\n",
      "Iter  40 | Gap: 1.47e-04 | Primal Loss: 0.615818\n",
      "Iter  50 | Gap: 1.30e-04 | Primal Loss: 0.615815\n"
     ]
    }
   ],
   "source": [
    "classifier = SDCAClassifier(loss=\"log\", lam=1e-3, max_iter=5000000, verbose=1, tol=1e-5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb3c05-20d6-4a66-b235-d3472c999054",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(classifier.losses_)\n",
    "plt.xlabel(\"Iteration (every 10 steps)\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"SCDA Training Loss Curve in Log Scale\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5336af0-eff7-44f3-a0a0-3b61e596fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(classifier.losses_)\n",
    "plt.xlabel(\"Iteration (every 10 steps)\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"SCDA Training Loss Curve\")\n",
    "#plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65b0b4-f801-4c67-9863-54a6ba650f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.losses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f9d3509-aacf-441f-8f10-5c3ed2b4941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7887869069998894\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "149b31e3-a3d6-40df-97c4-5bd72487cbc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SDCAClassifier' object has no attribute 'bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_loss)\n",
      "Cell \u001b[1;32mIn[8], line 116\u001b[0m, in \u001b[0;36mSDCAClassifier._compute_loss\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m--> 116\u001b[0m     pred \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m         z \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m pred\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SDCAClassifier' object has no attribute 'bias'"
     ]
    }
   ],
   "source": [
    "test_loss = classifier._compute_loss(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a64c95-97e9-4502-ae1c-1947c316febb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
