{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb518ed7-0cff-4b2d-88a7-422a1b6f9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SDCAClassifier:\n",
    "    def __init__(self, loss=\"squared\", lam=1e-3, max_iter=1500, tol=1e-5, verbose=False):\n",
    "        self.loss = loss\n",
    "        self.lam = lam\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.w = None\n",
    "        self.alpha = None\n",
    "        self.losses_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Important: initialize alpha based on the loss requirements\n",
    "        self.alpha = np.zeros(n_samples) \n",
    "        self.w = np.zeros(n_features)\n",
    "        # random initialization \n",
    "        #self.alpha = np.random.randn(n_samples) * 0.01\n",
    "        #using the formula of w in terms of alpha to initialize\n",
    "        #self.w = (1 / (self.lam * n_samples)) * X.T @ self.alpha\n",
    "\n",
    "\n",
    "        y_internal = y.copy()\n",
    "        # Internal labels must be {-1, 1} for the dual conjugates used here\n",
    "        if self.loss == \"log\":\n",
    "            y_internal = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        \n",
    "        for it in range(self.max_iter):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            for i in indices:\n",
    "                xi = X[i]\n",
    "                yi = y_internal[i]\n",
    "                norm_sq = np.dot(xi, xi)\n",
    "                if norm_sq == 0: continue\n",
    "\n",
    "                pred = np.dot(self.w, xi)\n",
    "                \n",
    "                # Denominator for coordinate update\n",
    "                # This represents the curvature of the objective\n",
    "                denom = norm_sq / (self.lam * n_samples) + 1\n",
    "                \n",
    "                if self.loss == \"squared\":\n",
    "                    # For squared loss, the optimal update is closed-form\n",
    "                    delta = (yi - pred - self.alpha[i]) / denom\n",
    "                \n",
    "                elif self.loss == \"log\":\n",
    "                    # For logistic, we use a Newton step\n",
    "                    # Dual variable alpha_i for yi*u should be in [0, 1]\n",
    "                    # Here we track alpha such that w = 1/(lam*n) sum alpha_i * xi\n",
    "                    exp_term = np.exp(np.clip(yi * pred, -50, 50))\n",
    "                    grad = yi / (1 + exp_term) - self.alpha[i]\n",
    "                    delta = grad / denom\n",
    "                    \n",
    "                    # Projection: for logistic, yi * alpha_i must be in [0, 1]\n",
    "                    # This is equivalent to alpha_i being between 0 and yi\n",
    "                    new_alpha = np.clip(self.alpha[i] + delta, min(0, yi), max(0, yi))\n",
    "                    delta = new_alpha - self.alpha[i]\n",
    "\n",
    "                self.alpha[i] += delta\n",
    "                # Incrementally update w to keep it in sync with alpha\n",
    "                self.w += (delta / (self.lam * n_samples)) * xi\n",
    "            \n",
    "            # Recalculate metrics to check convergence\n",
    "            primal_obj, dual_obj = self._compute_objectives(X, y_internal)\n",
    "            gap = primal_obj - dual_obj\n",
    "            self.losses_.append(primal_obj)\n",
    "\n",
    "            if self.verbose and (it + 1) % 10 == 0:\n",
    "                # We use Relative Gap to see progress better\n",
    "                rel_gap = gap / max(abs(primal_obj), 1.0)\n",
    "                #print(f\"Iter {it+1:3d} | Primal: {primal_obj:.5f} | Gap: {gap:.2e} | RelGap: {rel_gap:.2%} | Loss: {self.loss}\")\n",
    "                print(f\"Iter {it+1:3d} | Gap: {gap:.2e} | Primal Loss: {primal_obj:.6f}\")\n",
    "            \n",
    "            if gap < self.tol:\n",
    "                if self.verbose: print(f\"Converged at iteration {it+1}\")\n",
    "                break\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def _compute_objectives(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        preds = X @ self.w\n",
    "        \n",
    "        # 1. Primal Objective\n",
    "        if self.loss == \"squared\":\n",
    "            loss_term = np.mean(0.5 * (preds - y)**2)\n",
    "        else:\n",
    "            loss_term = np.mean(np.log(1 + np.exp(-y * preds)))\n",
    "        primal_obj = loss_term + 0.5 * self.lam * np.dot(self.w, self.w)\n",
    "\n",
    "        # 2. Dual Objective (Fenchel Dual)\n",
    "        # Using the conjugate: phi*(v) = 0.5*v^2 + yi*v for squared\n",
    "        # Using the conjugate: phi*(v) = v*log(v) + (1-v)*log(1-v) for logistic\n",
    "        if self.loss == \"squared\":\n",
    "            dual_res = np.mean(0.5 * self.alpha**2 - y * self.alpha)\n",
    "        else:\n",
    "            # For logistic, v = yi * alpha_i must be in [0, 1]\n",
    "            v = np.clip(y * self.alpha, 1e-12, 1 - 1e-12)\n",
    "            dual_res = np.mean(v * np.log(v) + (1 - v) * np.log(1 - v))\n",
    "            \n",
    "        dual_obj = -dual_res - 0.5 * self.lam * np.dot(self.w, self.w)\n",
    "        return primal_obj, dual_obj\n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        pred = X @ self.w\n",
    "        if self.loss == \"squared\":\n",
    "            loss = np.mean(0.5*(pred - y)**2) + 0.5 * self.lam * np.linalg.norm(self.w)**2\n",
    "        elif self.loss == \"log\":\n",
    "            z = y * pred\n",
    "            loss = np.mean(np.log(1 + np.exp(-z))) + 0.5 * self.lam * np.linalg.norm(self.w)**2\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (X @ self.w > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12c0a74-997e-4ae3-b9c5-3f2e9d67d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset\n",
    "bank_marketing = fetch_ucirepo(id=222)\n",
    "X = bank_marketing.data.features\n",
    "y = (bank_marketing.data.targets == \"yes\").astype(int)  # convert yes/no -> 1/0\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y.values.ravel(), test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50dba40-87af-4b03-bf0e-4f769cbcd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab92ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 1.0e+00 - Lambda 1.0e-01 - Test Loss: 0.044916- Steps 4 - Accuracy 0.753621585756939\n",
      "LR 1.0e+00 - Lambda 1.0e-02 - Test Loss: 0.043791- Steps 18 - Accuracy 0.75494857901139\n",
      "LR 1.0e+00 - Lambda 1.0e-03 - Test Loss: 0.043669- Steps 109 - Accuracy 0.7556120756386155\n",
      "LR 1.0e+00 - Lambda 1.0e-04 - Test Loss: 0.043658- Steps 909 - Accuracy 0.7552803273250027\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lr_set = [1]\n",
    "lams = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "lams.reverse()\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for lr in lr_set:\n",
    "    for lam in lams:\n",
    "        classifier = SDCAClassifier(loss=\"squared\", lam=lam, verbose=0, tol=1e-6)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        test_loss = classifier._compute_loss(X_test, y_test)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"LR {lr:.1e} - Lambda {lam:.1e} - Test Loss: {test_loss:.6f}- Steps {len(classifier.losses_)} - Accuracy {accuracy}\")\n",
    "        \n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_params = (lr, lam) \n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(\"LR:\", best_params[0], \"Lambda:\", best_params[1], \"Test Loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12fc91a7-42c4-499c-8d57-75a22114041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  10 | Gap: 2.85e-04 | Primal Loss: 0.042853\n",
      "Iter  20 | Gap: 1.48e-05 | Primal Loss: 0.042717\n",
      "Converged at iteration 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SDCAClassifier at 0x1db37769db0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SDCAClassifier(loss=\"squared\", lam=1e-3, max_iter=5000, verbose=1, tol=1e-5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb3c05-20d6-4a66-b235-d3472c999054",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(classifier.losses_)\n",
    "plt.xlabel(\"Iteration (every 10 steps)\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"SCDA Training Loss Curve in Log Scale\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5336af0-eff7-44f3-a0a0-3b61e596fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(classifier.losses_)\n",
    "plt.xlabel(\"Iteration (every 10 steps)\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"SCDA Training Loss Curve\")\n",
    "#plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65b0b4-f801-4c67-9863-54a6ba650f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.losses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9d3509-aacf-441f-8f10-5c3ed2b4941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7545062479265731\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b31e3-a3d6-40df-97c4-5bd72487cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = classifier._compute_loss(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a64c95-97e9-4502-ae1c-1947c316febb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
